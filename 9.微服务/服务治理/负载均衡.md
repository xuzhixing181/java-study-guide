# 初始负载均衡

- 负载：可理解为要处理的工作和压力，比如：网络请求、事务、数据处理任务等
- 均衡：将工作和压力平均地分配给多个工作者，从而分摊每个工作者的压力，以保证大家可正常工作
- 负载均衡：用于将请求分发到多个服务节点上的技术，以确保每个资源都能有效地处理负载、增加系统的并发量、避免某些资源过载而导致性能下降或服务不可用的情况（即便部分服务器出现故障，系统仍然可继续提供服务，以提高系统的整体可用性和可靠性；可避免单个服务器因负载过重而成为瓶颈，从而提升系统的整体性能）
- 对于RPC框架来说，负载均衡的作用是从一组可用的服务提供者中选择其一进行调用
- 常用的负载均衡技术有Nginx（七层负载均衡）、LVS（四层负载均衡）等

# 负载均衡算法

- 负载均衡算法：即是选择资源的策略

- 应用场景不同，选择的负载均衡算法可能也会有所不同，一定要根据实际情况选取，主流的负载均衡算法如下：

  1）轮询（Round Robin）：按照循环的顺序将请求分配给每个服务器，适用于各个服务器性能相近的情况

  2）随机（Random）：随机选择一个服务器来处理请求，适用于服务器性能相近且负载均匀的情况

  3）加权轮询（Weighted Round Robin）：根据服务器的性能或权重来分配请求，性能更好的服务器能获取到更多的请求，适用于服务器性能不均的情况

  4）加权随机（Weighted Random）：根据服务器的权重随机选择一个服务器来处理请求，适用于服务器性能不均的情况

  5）最小连接数（Least Connection）：选择当前连接数最少的服务器来处理请求，适用于长连接场景

## 一致性Hash

- 一致性哈希算法：经典的Hash算法，用于将请求分配到多个节点或服务器上，非常适用于负载均衡
- 核心思想：将整个哈希值空间划分成一个环状结构，每个节点或服务器在环上占据一个位置，每个请求根据其哈希值映射到环上的一个位置，其后再顺时针寻找第一个大于或等于该哈希值的节点，将请求路由到该节点上

![image](https://github.com/user-attachments/assets/7df05e44-7d6e-4b49-ac8f-1c93c04150f3)


- 一致性哈希之所以采用环状结构，是因为还解决了**节点下线**和**请求倾斜**的问题

- **节点下线**：当某节点下线时，其负载会被平均分摊到其他节点上，而不会影响到整个系统的稳定性，因为只有部分请求会受到影响

  - 如下图所示，服务器C下线后，请求A会交给服务器A来处理（顺时针寻找到第一个大于或等于该哈希值的节点），而服务器B接收到的请求会保持不变
  - 如果采用轮询取模算法，只要节点数发生变化，很有可能大多数服务器处理的请求都要发生变化，对系统的影响巨大

  ![image-20250216150304373](C:\Users\xyl\AppData\Roaming\Typora\typora-user-images\image-20250216150304373.png)

- 负载倾斜：通过虚拟节点的引入，将每个物理节点映射到多个虚拟节点上，使得节点在哈希环上的分布更加均匀，减少了节点间负载的差异

  ![image-20250216151912797](C:\Users\xyl\AppData\Roaming\Typora\typora-user-images\image-20250216151912797.png)

- 当节点很少时，环的情况如上所示，会导致大多数的请求都会发给服务器C，而发给服务器A的请求很少，甚至几乎不会有请求，引入虚拟节点后，环的情况如下（服务器接收到的请求就会变得更加平均）：


![image](https://github.com/user-attachments/assets/979aea7b-66ef-4821-9ca1-b669156c4f21)


- 关于哈希算法，需要知道哈希算法的基本步骤，哈希类负载均衡算法的弊端，数据倾斜及其对应的解决方案

![image](https://github.com/user-attachments/assets/2e143c78-e2c7-4a74-a19c-4d5a1edd55f8)


- 设计哈希类负载均衡器大致需要考虑如下三个问题：

  - 1）确定哈希键：选择一个哈希的哈希键，用于计算哈希值，常见的选择有：

    - 客户端IP地址：用于保持会话亲和性
    - 请求URL：用于内容分发网络（CDN）或缓存系统
    - 用户ID/会话ID：用于确保用户请求的一致性

  - 2）选择哈希函数：选择合适的哈希函数来计算哈希值，合适的哈希函数应具备以下特征：

    - 均匀性：能将输入均匀地分布到输出空间
    - 快速性：计算速度快，不会成为系统瓶颈
    - 确定性：相同的输入总会产生相同的输出

    常见的哈希函数包括MD5、SHA-1、SHA-256，以及更简单的函数（如：CRC32）或自定义的哈希函数

    如果哈希键本身就是数字（如：自增主键），就可直接用来作为哈希值，不需要进行任何转换

  - 3）映射哈希值到服务器：将计算得到的哈希值映射到具体的服务器上，常见方法如下：

    - 直接映射：将哈希值对服务器数量取模，得到服务器的索引，serverIndex = hashValue % serverSize
    - 一致性哈希：将服务器和请求都映射到一个虚拟的哈希环上，请求顺时针找到的第一个服务器就是目标服务器，如此在服务器数量发生变化时，不会造成大量请求的重新映射
    - 自定义映射：通过静态配置等方式，指定符合某个条件的哈希值能映射到某个服务器

## 编码实现

- 1）提供一个通用的负载均衡器接口，用于定义选择服务节点的规范

```java
import java.util.List;
import java.util.Map;

/**
 * 通用的负载均衡器接口
 * @author https://github.com/xuzhixing181
 */
public interface LoadBalancer {

    /**
     * 选择服务器节点
     * @requestParams: 请求参数
     * @serviceMetaInfoList: 可用的服务实例列表
     */
    public ServiceMetaInfo selectServer(Map<String,Object> requestParams, List<ServiceMetaInfo> serviceMetaInfoList);
}
```

- 轮询负载均衡器：

```java
import java.util.List;
import java.util.Map;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * 轮询负载均衡器
 * @author https://github.com/xuzhixing181
 */
public class RoundRobinLoadBalancer implements LoadBalancer{

    private AtomicInteger curIndex = new AtomicInteger(0);
    @Override
    public ServiceMetaInfo selectServer(Map<String, Object> requestParams, List<ServiceMetaInfo> serviceMetaInfoList) {
        if (serviceMetaInfoList == null || serviceMetaInfoList.isEmpty()){
            return null;
        }
        int serverSize = serviceMetaInfoList.size();
        if(serverSize == 1) return serviceMetaInfoList.get(0);
        int nextIndex = curIndex.getAndIncrement() % serverSize;
        return serviceMetaInfoList.get(nextIndex);
    }
}
```

- 随机负载均衡器：

```java
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * 随机负载均衡器
 * @author https://github.com/xuzhixing181
 */
public class RandomLoadBalancer implements LoadBalancer {

    @Override
    public ServiceMetaInfo selectServer(Map<String, Object> requestParams, List<ServiceMetaInfo> serviceMetaInfoList) {
        if (serviceMetaInfoList ==null || serviceMetaInfoList.size() == 0){
            return null;
        }
        int serverSize = serviceMetaInfoList.size();
        if(serverSize == 1){
            return serviceMetaInfoList.get(0);
        }
        return serviceMetaInfoList.get(new Random().nextInt(serverSize));
    }
}
```

- 一致性哈希负载均衡器：

```java
import java.util.List;
import java.util.Map;
import java.util.TreeMap;

/**
 * 一致性哈希负载均衡器
 * @author https://github.com/xuzhixing181
 */
public class ConsistentHashLoadBalancer implements Loadbalancer{

    // 一致性hash环,存放虚拟节点
    private TreeMap<Integer,ServiceMetaInfo> CONSISTENT_HASH_MAP = new TreeMap<>();

    // 虚拟节数
    private static final int VIRTUAL_NODE_COUNT = 100;

    @Override
    public ServiceMetaInfo selectServer(Map<String, Object> requestParams, List<ServiceMetaInfo> serviceMetaInfoList) {
        if (serviceMetaInfoList == null || serviceMetaInfoList.isEmpty()){
            return null;
        }
        // 1.构建虚拟节点环
        for (ServiceMetaInfo serviceMetaInfo : serviceMetaInfoList) {
            for (int i = 0; i < VIRTUAL_NODE_COUNT; i++) {
                int hash = getHash(serviceMetaInfo.getServiceAddress() + "#" + i);
                CONSISTENT_HASH_MAP.put(hash,serviceMetaInfo);
            }
        }
        // 2.获取调用调用的请求hash值
        int curHash = getHash(requestParams);

        // 3.选择最接近且大于等于请求hash值的虚拟节点
        Map.Entry<Integer,ServiceMetaInfo> metaInfoEntry = CONSISTENT_HASH_MAP.ceilingEntry(curHash);
        if (metaInfoEntry == null){
            metaInfoEntry = CONSISTENT_HASH_MAP.firstEntry();
        }
        // 获取一致性Hash环中的某个服务节点
        return metaInfoEntry.getValue();
    }

    private int getHash(Object key){
        return key.hashCode();
    }
}
```

## 存在问题及解决方案

### 负载不均衡

- 在最开始阶段，为提高本地缓存命中率，可使用哈希负载均衡算法，以确保同一个用户的数据一直落在同一个节点上，具体来说，根据用户ID计算哈希值，并分配到1025个槽上，而后将这1024个槽平均分配到所有节点上

  <img src="C:\Users\xyl\AppData\Roaming\Typora\typora-user-images\image-20250216132342215.png" alt="image-20250216132342215" style="zoom: 50%;" />

- 如果有部分用户的热点请求落在某个节点，就可能会导致该节点要比其他节点的负载要更高，

  为了解决这个问题，就引入了动态分配槽的算法，且每分钟重新分配一次。最开始的时候都是平均分配的，但是在运行期间，通过一个数组，维护住每个槽的请求数量。而后统计全部槽的请求总数，除以节点数量，就是预期中每个节点上应该分配的请求数量。

接下来就是要用动态规划算法，动态分配槽，将0~1024个槽切分成段（如：A、B、C段），且确保 A、B、C每段上的请求总数量尽可能地接近平均值，并且差值的绝对值的和应该最小
具体来说，状态定义dp[i][k]表示前i个槽分成k段的最小总绝对差。状态转移方程为：
`dp[i][k] = min{ dp[j][k-1] + |sum[j+1,i] - avg| }` (j从k-1到i-1)
为控制时间复杂度：

- 预处理前缀和数组加速区间和计算
- 限制最大分段数（如节点数±20%），缩小状态空间
- 采用滚动数组优化空间复杂度到O(n)
  应对剧烈波动：
- 设定分段请求数容差阈值（如±15%），超出时触发告警人工介入
- 增加历史滑动窗口（如取最近3分钟均值）平滑突发流量"

但如上的算法也无法彻底解决负载均衡的问题，因为有两个根本问题无法解决：

- 1）无法准确地实时地计算出服务器的负载
- 2）无法准确地提前计算出处理请求所需资源。

   两者一结合会导致，不管选择什么样的负载均衡算法，都可能引起负载不均衡问题。



这种算法的好处是它完全是动态的，自适应的，规避了静态分配无法适应流量变化的问题

- 哈希负载均衡算法有一个很严重的问题，就是可能出现热点问题或者数据倾斜问题。比如说经过计算哈希值、服务器映射之后，某个服务器的负载要比其它服务器明显地高，这种可以通过重新选择哈希算法或者重建服务器映射来解决。

  其次，哈希负载均衡算法没有考虑服务器的性能问题。当然，这个问题可以在映射到服务器的过程中解决。比如说一致性哈希负载均衡里面，可以映射更多的虚拟服务器给物理服务器。



# 应用场景

## 提高本地缓存的命中率

- 为提高本地缓存的命中率，我使用了一致性哈希的负载均衡算法，将同一个用户请求的数据打到同一个节点上，最开始时，我使用的是固定分配的哈希负载均衡算法，简单来说，即是将哈希值分成1024个槽，而后1024个槽平均分配到所有节点上（如常规情况下有9台实例，即每个实例复杂114个槽）
- 但有时会有部分热点数据集中在其中一个实例上，就会导致该实例的负载比其他实例高，这种情况下，我设计了动态分配槽的算法，每分钟重新分配一次
  - 同样是1024个槽，维持住每个槽对应的请求数，而后每分钟计算一下总请求数，除以节点数量就是预期中每个节点应该分配到的请求数量
  - 用动态规划，将0~1024个槽切分成段，并确保每段内的请求总数都接近平均值，每段的请求总数减去平均值的差值的绝对值的和最小
  - 这种算法的好处是完全动态的，自适应的，规避了静态分配无法适应流量变化的问题

- 动态映射服务器的一致性哈希负载均衡，即根据实际的请求数量，动态调整一致性哈希算法中的虚拟服务器到物理服务器的映射，有效解决数据倾斜和热点问题



##  Nginx

- Nginx 配置文件使用类似于代码的结构化语法，专门用于设置服务器行为、路由规则等，在 Web 服务部署中广泛使用，可实现负载均衡、反向代理等多种功能

- Nginx中一致性哈希是这样实现的：

  - 1）确定哈希环的长度，2^32
  - 2）映射节点到哈希环，Nginx中节点指的是虚拟节点，默认情况下，物理节点与虚拟节点的映射关系是1:160，源码如下：

  1） 确定哈希环的长度，2^32：

```C
for (i = 0; i < 160; i++) {
    // 使用物理节点的名字及名字长度构造基本哈希，物理节点名一般来自用户配置，见下方配置。  
    hash = ngx_crc32_short(server->name.data, server->name.len); 
    // 利用物理节点的基本哈希及虚拟节点的编号i来构造出虚拟节点在哈希环上的点位，间接构地表达出虚拟节点与物理节点的映射关系
    // ngx_crc32_short的结果总是一个32位无符号整数，这与对2^32取模的结果范围相同。
    hash = ngx_crc32_short((u_char *) &i, 4, hash);  
    // ......
}  
```

2）映射数据到哈希环，具体要将什么数据映射到哈希环上，需要在Nginx的配置中指出：

```nginx
http {  
    upstream backend {  
        # 启用了一致性哈希，其中 $request_uri 是用作哈希键的变量
        # consistent 关键字指定使用一致性哈希算法
        hash $request_uri consistent;
        # 物理节点名称  
        server backend1.example.com;  
        server backend2.example.com;  
        server backend3.example.com;  
    }  
    server {  
        listen 80;  
        location / {  
            proxy_pass http://backend;  
        }  
    }  
}  
```

- 通过数据寻找节点，从数据在哈希环上的位置出发沿着顺时针找节点，当你向Nginx发送请求时，Nginx就会根据算法转发到找到的虚拟节点对应的后端物理节点了。



## 数据分片

在Redis中，一致性哈希用来实现Redis Cluster功能：

1）确定哈希环的长度，2^14 = 16384

2）**映射节点到哈希环，Redis中的节点也是指虚拟节点**，又被称作哈希槽（slot）。因为哈希环的长度是16384，Redis中就有16384个哈希槽（即虚拟节点），至于物理节点（Redis实例）与虚拟节点（哈希槽）的映射关系你可以通过cluster create命令委托给Redis，Redis在创建集群的过程中会将哈希槽平均分布在各个实例时，如果有N个实例，那么每个实例与16384/N个哈希槽关联。你也可以使用 cluster addslots 命令手动建立关联，但要将16384个哈希槽全部关联完否则集群无法启动工作。注意，在Redis的语境下会使用“**将哈希槽分配给Redis实例**”的表达，其实等价于“**虚拟节点与物理节点之间的映射关系**”

3）**映射数据到哈希环，Redis中会根据键值对的 key，通过CRC16哈希函数计算得到一个16 bit的哈希值**；比如：hash = crc16(key)

4）通过数据寻找节点，用步骤3中得到的哈希值对16384取模得到哈希槽的编号，比如: index = hash%16384, 其中hash = crc16(key)。实际上Redis的源码中用的是crc16(key)&16383等价于crc16(key)%16384,但运行速度更快

# 模拟面试

- 问：使用一致性哈希算法后，还会有负载不均衡的问题吗？

  答：

  > 一致性哈希算法无法彻底解决负载均衡的问题，两个根本原因如下：
  >
  > - 1）无法准确地实时计算出服务器的负载
  > - 2）无法准确地提前计算出处理请求所需的资源
  >
  > 基于如上两个原因，不管选择何种算法，都可能会引起负载不均衡的问题

- 问：为什么你选择 1 分钟调整一次？可以更快或者更慢吗？

  答：

  > 选择 1 分钟是一个经验值。首先算法本身是比较复杂的，所以性能损耗很大，应该尽量减少调整的次数。其次，大部分系统能撑住 1 分钟的负载不均衡。

  > 如果要是流量变动非常频繁，比如说某些槽上的请求一会多一会少，变得很快，那么就减少调整的间隔，比如说调整到 3s 一次。如果要是流量变动不大，那么可以调长一点
